{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import re\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./stored_authors/author_url_pairings.txt\", encoding=\"utf8\") as pairings_file:\n",
    "    author_url_pairings = eval(pairings_file.read())\n",
    "with open(\"./stored_authors/unknown_authors.txt\", encoding=\"utf8\") as unknowns_file:\n",
    "    unknown_authors = eval(unknowns_file.read())\n",
    "with open(\"./stored_authors/duplicate_authors.txt\", encoding=\"utf8\") as duplicates_file:\n",
    "    duplicate_authors = eval(duplicates_file.read())\n",
    "with open(\"./stored_authors/authors_ids.txt\", encoding=\"utf8\") as authors_ids_file:\n",
    "    authors_ids = eval(authors_ids_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for reference when checking for authors in duplicate list\n",
    "\n",
    "unpacked_duplicates = set()\n",
    "for author_list in duplicate_authors.values():\n",
    "    for author in author_list:\n",
    "        unpacked_duplicates.add(author)\n",
    "        \n",
    "sorted_unpacked_duplicates = sorted(list(unpacked_duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_likely = [] # Highly likely, unknowns that have matches\n",
    "possible = [] # Moderately likely, the ones whose possible matches have similar characters\n",
    "unlikely = [] # Unlikely, few matching characters\n",
    "\n",
    "# Pairs duplicates\n",
    "duplicate_pairing = {}\n",
    "for author, url in author_url_pairings.items():\n",
    "    if author in sorted_unpacked_duplicates:\n",
    "        duplicate_pairing[url] = []\n",
    "        \n",
    "# Iterates through duplicates and appends pairings for each URL\n",
    "for author, pair in duplicate_authors.items():\n",
    "    for pair_id in pair:\n",
    "        duplicate_pairing[author_url_pairings[pair_id]].append(pair_id)\n",
    "        \n",
    "merged_pairing = deepcopy(duplicate_pairing)\n",
    "\n",
    "# Generates list of all the url IDs for any urls that have them (i.e. urls with \"contributions\"\n",
    "url_num_list = []\n",
    "for url in duplicate_pairing:\n",
    "    if \"contributions\" in url:\n",
    "        url_num_list.append(re.search(r\"\\d+\", url).group(0))\n",
    "\n",
    "# Generates list for all url IDs that have > 1 urls that contain them\n",
    "same_num_list = [num for num, count in Counter(url_num_list).items() if count > 1]\n",
    "\n",
    "# Iterates through nums then urls: longest_url -> longest (most complete name) to set as the new merged key\n",
    "# merged_entries -> merging of the values of the keys for assigning to the new merged key \n",
    "for num in same_num_list:\n",
    "    longest_url = \"\"        # Will be used as the key of the new merged entry (longer name is almost always the most detailed name)\n",
    "    url_list = []           # Keeps track of urls to remove them from merged_pairing \n",
    "    merged_entries = []     # Used a value for new merged entry\n",
    "    \n",
    "    # Finds values and key for merging values\n",
    "    for url in duplicate_pairing:\n",
    "        if num in url:\n",
    "            url_list.append(url)\n",
    "            merged_entries += duplicate_pairing[url]\n",
    "            \n",
    "            # Assigns longest_url for use as a key\n",
    "            if len(url) > len(longest_url):\n",
    "                longest_url = url\n",
    "    \n",
    "    # Merges/assigns values import itertools\n",
    "    for url in url_list:\n",
    "        if url == longest_url:\n",
    "            merged_pairing[url] = merged_entries\n",
    "            \n",
    "        # Removes other unmerged entries\n",
    "        else:\n",
    "            merged_pairing.pop(url)\n",
    "\n",
    "for url, matches in merged_pairing.items():\n",
    "    if len(matches) > 1:\n",
    "        h_likely.append(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairs unknown names (e.g. first initial names)\n",
    "\n",
    "unknown_pairing = {}\n",
    "for unknown in unknown_authors:\n",
    "    unknown_pairing[unknown] = None\n",
    "\n",
    "for author, pair in unknown_authors.items():\n",
    "\n",
    "    if author in author_url_pairings:\n",
    "        author_url = author_url_pairings[author]\n",
    "    else:\n",
    "        author_url = \"no\"\n",
    "\n",
    "#     print(\"-----new iteration-----\")\n",
    "#     print(author)\n",
    "#     print(author_url) # \n",
    "#     print(\"--end of author url--\")\n",
    "\n",
    "    found = \"Unsure\"\n",
    "    for pair_id in pair:\n",
    "        pair_url = author_url_pairings[pair_id]\n",
    "\n",
    "#         print(pair_url) #\n",
    "\n",
    "        if \"contributions\" in author_url and \"contributions\" in pair_url:\n",
    "            author_num = re.search(r\"\\d+\", author_url).group(0)\n",
    "            pair_num = re.search(r\"\\d+\", pair_url).group(0)\n",
    "            if author_num == pair_num:\n",
    "                found = pair_id\n",
    "                h_likely.append([author, pair_id])\n",
    "\n",
    "#                 print(\"yes!\") #\n",
    "\n",
    "        elif author_url ==  pair_url:\n",
    "            found = pair_id\n",
    "            h_likely.append([author, pair_id])\n",
    "\n",
    "#             print(\"yes!\") #\n",
    "\n",
    "    unknown_pairing[author] = found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# for list_thing in h_likely:\n",
    "#     if len(list_thing) == 2:\n",
    "#         print(list_thing)\n",
    "\n",
    "all_h_likely_values = list(itertools.chain(*h_likely))\n",
    "# pprint(h_first)\n",
    "h_likely_duplicates = [author for author, count in Counter(all_h_likely_values).items() if count > 1]\n",
    "# pprint(h_likely_duplicates)\n",
    "print(len(h_likely_duplicates))\n",
    "print(len(h_likely))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 92\n",
      "Not unsure: 0\n",
      "Unsure: 92\n"
     ]
    }
   ],
   "source": [
    "unknown_total = len(unknown_pairing)\n",
    "print(\"Total: \" + str(unknown_total))\n",
    "\n",
    "unknown_not_unsure = len([author for author, status in unknown_pairing.items() if status != \"Unsure\"])\n",
    "print(\"Not unsure: \" + str(unknown_not_unsure))\n",
    "\n",
    "unknown_unsure = unknown_total - unknown_not_unsure\n",
    "print(\"Unsure: \" + str(unknown_unsure))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 75\n",
      "Matches: 6\n",
      "Individuals (unsure): 69\n"
     ]
    }
   ],
   "source": [
    "duplicate_total = len(merged_pairing)\n",
    "print(\"Total: \" + str(duplicate_total))\n",
    "\n",
    "duplicate_matches = len([url for url, matches in merged_pairing.items() if len(matches) > 1])\n",
    "print(\"Matches: \" + str(duplicate_matches))\n",
    "\n",
    "duplicate_individual = duplicate_total - duplicate_matches\n",
    "print(\"Individuals (unsure): \" + str(duplicate_individual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Total: 167\n",
      "Overall highly_likely: 6\n",
      "Overall unsure: 161\n"
     ]
    }
   ],
   "source": [
    "overall_total = unknown_total + duplicate_total\n",
    "print(\"Overall Total: \" + str(overall_total)) \n",
    "\n",
    "overall_highly_likely = len(h_likely)\n",
    "print(\"Overall highly_likely: \" + str(overall_highly_likely))\n",
    "\n",
    "overall_unsure = overall_total - overall_highly_likely\n",
    "print(\"Overall unsure: \" + str(overall_unsure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects an author as the \"root\" (the longest/one with the most special characters)\n",
    "\n",
    "processing_roots = {}\n",
    "root_names = {}\n",
    "\n",
    "# Iterates through the lists in h_likely\n",
    "for matches in h_likely:\n",
    "    root_name = \"\"              # Longest/most accurate name to have everything merge into for database\n",
    "    root_id = \"\"                # ID of the root author\n",
    "    to_be_merged = deepcopy(matches) # remember to remove the root from matches\n",
    "    \n",
    "#     print(to_be_merged) #\n",
    "    \n",
    "    # For every author_id in the matches list\n",
    "    for author in matches: \n",
    "        full_name = authors_ids[author][0] + \"_\" + authors_ids[author][1] + \"_\" + authors_ids[author][2]\n",
    "        \n",
    "#         pprint(full_name) #\n",
    "#         print(author) #\n",
    "        \n",
    "        if len(author) == 19: # i.e. if the author_id is an ORC ID since ORC IDs are 19 characters while regular ID hashes are 40 characters\n",
    "            if root_id == \"\":\n",
    "                root_id = author\n",
    "            else: # If there is a different ORC ID already (two different orc id == issue)\n",
    "                print(\"***** THESE ARE NOT THE SAME PERSON *****\")\n",
    "            \n",
    "        if len(full_name) > len(root_name):\n",
    "            root_name = full_name\n",
    "            temp_id = author\n",
    "    \n",
    "    if root_id == \"\": \n",
    "        root_id = temp_id\n",
    "    \n",
    "#     print(root_id) #\n",
    "    \n",
    "    to_be_merged.remove(root_id)\n",
    "    processing_roots[root_id] = to_be_merged\n",
    "    root_names[root_id] = root_name\n",
    "\n",
    "# pprint(processing_roots)\n",
    "\n",
    "with open(\"./stored_authors/root_names.txt\", 'w') as root_out:\n",
    "    pprint(root_names, stream = root_out)\n",
    "with open(\"./stored_authors/mergees.txt\", 'w') as mergees_out:\n",
    "    pprint(processing_roots, stream = mergees_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(processing_roots))\n",
    "print(len(h_likely))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0000-0001-6082-5862': ['4ca5f2776a525fdab98d81b63136522f5ae6fddb'],\n",
      " '0000-0002-4662-8448': ['a7c760ed9fa9e1231fb26de6e4165f1e8821513c'],\n",
      " '0000-0003-0967-6560': ['3c58c50e830dd7b775bf193163c4fbd2411ea2f5'],\n",
      " '778f4eb01ada8ecbadfa975f2af019500f6685de': ['c5d18fa34d9d580c8e24e1ea34035555fb22356d'],\n",
      " '975de28b0249c30fde13e4d1214d21d5d4ac6763': ['4ff206651a890dafc062f9f7fa51888d77b2bca3'],\n",
      " 'a6ec8ac36e50d6b37057a103b0ef45aca4c8c5c5': ['64dbff6304e5a7fb80784e678eb6e19464791f80']}\n"
     ]
    }
   ],
   "source": [
    "pprint(processing_roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0000-0001-6082-5862': 'Yves__Gensterblum',\n",
      " '0000-0002-4662-8448': 'Covadonga__Pevida',\n",
      " '0000-0003-0967-6560': 'Richard__Sakurovs',\n",
      " '778f4eb01ada8ecbadfa975f2af019500f6685de': 'David_C._Langreth',\n",
      " '975de28b0249c30fde13e4d1214d21d5d4ac6763': 'Bidyut_Baran_Saha',\n",
      " 'a6ec8ac36e50d6b37057a103b0ef45aca4c8c5c5': 'Petra_Ágota_Szilágyi'}\n"
     ]
    }
   ],
   "source": [
    "pprint(root_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0000-0001-6082-5862', '4ca5f2776a525fdab98d81b63136522f5ae6fddb'],\n",
      " ['0000-0002-4662-8448', 'a7c760ed9fa9e1231fb26de6e4165f1e8821513c'],\n",
      " ['0000-0003-0967-6560', '3c58c50e830dd7b775bf193163c4fbd2411ea2f5'],\n",
      " ['4ff206651a890dafc062f9f7fa51888d77b2bca3',\n",
      "  '975de28b0249c30fde13e4d1214d21d5d4ac6763'],\n",
      " ['778f4eb01ada8ecbadfa975f2af019500f6685de',\n",
      "  'c5d18fa34d9d580c8e24e1ea34035555fb22356d'],\n",
      " ['64dbff6304e5a7fb80784e678eb6e19464791f80',\n",
      "  'a6ec8ac36e50d6b37057a103b0ef45aca4c8c5c5']]\n"
     ]
    }
   ],
   "source": [
    "pprint(h_likely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(merged_pairing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
