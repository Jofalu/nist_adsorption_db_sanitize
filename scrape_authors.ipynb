{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import urllib\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_driver(url):\n",
    "    \"\"\"\n",
    "    Given a URL string, opens the URL in a headless Firefox instance. \n",
    "    \n",
    "    Returns a reference to the webdriver object\n",
    "    \"\"\"\n",
    "    \n",
    "    buttons = []\n",
    "    driver_options = Options()\n",
    "#     driver_options.headless = True\n",
    "    \n",
    "    print(\"\\n\\n*****\" + str(datetime.datetime.now()) + \"*****\")\n",
    "    \n",
    "    driver = webdriver.Firefox(options=driver_options, executable_path=GeckoDriverManager().install())\n",
    "    \n",
    "    # WARNING, THIS IS SPECIFIC TO THIS MACHINE (retrieves the installed uBlock Origin from the Firefox on this computer.) Need a computer-agnostic way.\n",
    "    driver.install_addon(\"~/.mozilla/firefox/f4t6w0s5.default-release/extensions/uBlock0@raymondhill.net.xpi\", temporary=True)\n",
    "\n",
    "#     driver.get(url)\n",
    "    \n",
    "    time.sleep(.5)\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_and_wait(driver, element, find_elements_by):\n",
    "    \"\"\" \n",
    "    Clicks on element and waits for a page load. \n",
    "    \n",
    "    Parameters:\n",
    "    driver == webdriver\n",
    "    element == string to find (class or css_selector)\n",
    "    find_elements_by == integer: 0 for css_selector, 1 for class_name.\n",
    "    \n",
    "    Only works on an actual page load (will hang if the click only runs dynamic JS on the same webpage)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        if find_elements_by == 0:\n",
    "            button = driver.find_element_by_css_selector(element)\n",
    "        else:\n",
    "            button = driver.find_element_by_class_name(element)\n",
    "            \n",
    "        driver.execute_script(\"arguments[0].click()\", button)\n",
    "    except:\n",
    "        print(\"No button\")\n",
    "        \n",
    "    old_driver = driver.find_element_by_tag_name('html')\n",
    "\n",
    "    WebDriverWait(driver, 10).until(EC.staleness_of(old_driver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_paper(doi, driver, engine):\n",
    "    \"\"\"\n",
    "    Given a DOI, an existing driver, and an engine number (0 for Google, 1 for DDG), search for paper.\n",
    "    \n",
    "    Returns the associated webdriver.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n---\" + \"New search \" + str(datetime.datetime.now()) + \"---\")\n",
    "    print(\"DOI: \" + doi)\n",
    "    \n",
    "    if engine == 0:\n",
    "        search_engine = \"https://www.google.com\"\n",
    "        # Constructs the string being searched, randomizes it so it's not so robotic\n",
    "        if random.randint(0, 1) == 0:\n",
    "            query_string = '\"' + doi + '\"' + ' ' + '\"researchgate.net\"'\n",
    "        else:\n",
    "            query_string = '\"researchgate.net\"' + ' ' + '\"' + doi + '\"'\n",
    "        query_string + \" -filetype:pdf\"\n",
    "            \n",
    "    elif engine == 1:\n",
    "        search_engine = \"https://duckduckgo.com/\"\n",
    "        query_string = '\\\\research gate' + ' ' + '\"' + doi + '\"'\n",
    "    \n",
    "    driver.get(search_engine)\n",
    "\n",
    "    \n",
    "    # Enters search string into searchbox\n",
    "    if engine == 0:\n",
    "        try: \n",
    "            blah = driver.find_element_by_xpath(\"/html/body/div/div[3]/form/div[2]/div/div[1]/div/div[1]/input\")\n",
    "        except NoSuchElementException: # Tries div[2]\n",
    "            print(\"Trying second xpath\")\n",
    "            blah = driver.find_element_by_xpath(\"/html/body/div/div[3]/form/div[2]/div/div[1]/div/div[2]/input\")\n",
    "        \n",
    "    \n",
    "    elif engine == 1:\n",
    "        blah = driver.find_element_by_xpath('//*[@id=\"search_form_input_homepage\"]')\n",
    "        \n",
    "    blah.send_keys(query_string)\n",
    "    \n",
    "    \n",
    "    time.sleep(random.randint(2, 7)) # to respect crawling\n",
    "    \n",
    "    if engine == 0:\n",
    "        # Clicks on \"I'm Feeling Lucky\" button\n",
    "        button = driver.find_element_by_xpath('//*[@id=\"gbqfbb\"]')\n",
    "        driver.execute_script(\"arguments[0].click()\", button)\n",
    "    elif engine == 1:\n",
    "        blah.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Waits for page to load\n",
    "    old_driver = driver.find_element_by_tag_name('html')\n",
    "    WebDriverWait(driver, 10).until(EC.staleness_of(old_driver))\n",
    "    time.sleep(2) # increased from .5 to 2 to respect crawling\n",
    "    \n",
    "    # Checks if paper was found\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    if \"/sorry/\" in driver.current_url:\n",
    "        raise Exception(\"Captcha'd\")\n",
    "        \n",
    "        return None\n",
    "    elif \"google.com\" in driver.current_url:\n",
    "        print(\"Paper not found\")\n",
    "        return None\n",
    "    else:\n",
    "        # Prints DOI string. Need to compare it to the actual doi to see if it's the correct paper.\n",
    "        print(\"Success: found at \" + str(driver.current_url))\n",
    "        return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_authors(driver): \n",
    "    \"\"\"\n",
    "    Given the webdriver, shows more authors. \n",
    "    \n",
    "    Returns the modified webdriver.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not driver:\n",
    "        print(\"Exiting: No driver\")\n",
    "        return None\n",
    "    \n",
    "    first_start = time.time()\n",
    "    \n",
    "    time.sleep(2) # Increase from .5 to 2 for stability\n",
    "    \n",
    "    try:\n",
    "        button = driver.find_element_by_xpath(\"/html/body/div[2]/main/section/section[1]/div[2]/a\")\n",
    "        driver.execute_script(\"arguments[0].click()\", button)\n",
    "    except:\n",
    "        print(\"No button\")\n",
    "        \n",
    "    time.sleep(1) # Increased to 1 for stability\n",
    "    \n",
    "    first_end = time.time()\n",
    "\n",
    "    print(\"DONE - \" + str(first_end-first_start))\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chars(string):\n",
    "    \"\"\"Removes - and space characters from a string\"\"\"\n",
    "    \n",
    "    return string.translate({ord(char): None for char in \"- \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_chars(string1, string2):\n",
    "    \"\"\"Counts the number of common characters in two strings\"\"\"\n",
    "    \n",
    "    common = Counter(string1.casefold()) & Counter(string2.casefold())\n",
    "    return sum(common.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_names(query_name, rg_name):\n",
    "    \"\"\"Compare names while agnostic to special characters and rearranged names\"\"\"\n",
    "    \n",
    "    # If one string is empty and not the other, return false\n",
    "    if (query_name == \"\" and rg_name != \"\") or (query_name != \"\" and rg_name == \"\"):\n",
    "        return False\n",
    "    \n",
    "    # Removes spaces and - from names\n",
    "    query_clean = remove_chars(query_name)\n",
    "    rg_clean = remove_chars(rg_name)\n",
    "    \n",
    "    # Same length and same amount of common characters\n",
    "    return len(query_clean) <= len(rg_clean) and common_chars(query_clean, rg_clean) == len(query_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_check(name):\n",
    "    \"\"\"Checks if name is an initial, returns true if so\"\"\"\n",
    "    \n",
    "    return (len(name) == 1) or (\".\" in name)\n",
    "# len(name) == 2 and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_authors(query_author, rg_author):\n",
    "    \"\"\"\n",
    "    Compares authors\n",
    "    \n",
    "    Parameters:\n",
    "    query_author : list of of queried author's first name then last name\n",
    "    rg_author    : string of researchgate author's full name\n",
    "    \"\"\"\n",
    "    \n",
    "    # Checks if rg_author has any special non-ASCII characters. Translates query_author based on that and sets the author's first and last name strings.\n",
    "    # Still doesn't address if one half of name uses UTF-8 only characters and the other half doesn't) but unlikely case\n",
    "    if unidecode(rg_author) == rg_author:\n",
    "        author_first = unidecode(query_author[0]).split()\n",
    "        author_last = unidecode(query_author[2]).split()\n",
    "    else:\n",
    "        author_first = query_author[0].split()\n",
    "        author_last = query_author[2].split()\n",
    "    \n",
    "    # Splits rg_author into tokens\n",
    "    rg_tokens = rg_author.split()\n",
    "    \n",
    "    # Removes Jr from last name. Need to put the last check in case someone is just named \"Jr\"\n",
    "    if len(rg_tokens) > 1 and rg_tokens[-1] == \"Jr\" or rg_tokens[-1] == \"Jr.\":\n",
    "        rg_tokens.pop(-1)\n",
    "    \n",
    "    # Deals with no first_name in query_author\n",
    "    if author_first == \"\" and compare_names(author_last, rg_tokens[-1]):\n",
    "        return True\n",
    "        \n",
    "    # Incase rg_author uses first name initial, compares the first letter of queried author's first name to that string\n",
    "    if initial_check(rg_tokens[0]):\n",
    "        author_first[0] = str(author_first[0][0]) + \".\"\n",
    "    \n",
    "    # Merges first name for queried author\n",
    "    merged_author_first = \"\"\n",
    "    for name in author_first:\n",
    "        \n",
    "        # Incase rg_author uses first name initial, compares the first letter of queried author's first name to that string\n",
    "        if initial_check(rg_tokens[0]):\n",
    "            name = name[0] + \".\"\n",
    "           \n",
    "        merged_author_first = merged_author_first + name\n",
    "        \n",
    "    # Assigns last name to last part of last name\n",
    "    author_last = author_last[-1]\n",
    "    \n",
    "    # Merges all but last name for researchgate name\n",
    "    merged_rg_first = \"\"\n",
    "    for name in rg_tokens[:-1]:\n",
    "        \n",
    "        # Adds periods to initials in the name\n",
    "        if len(name) == 1:\n",
    "            name = name + \".\"\n",
    "        \n",
    "        # Converts name to initial of queried author is in initial\n",
    "        if initial_check(author_first[0]):\n",
    "            name = name[0] + \".\"\n",
    "        \n",
    "        merged_rg_first = merged_rg_first + name\n",
    "        \n",
    "    return ( compare_names(merged_author_first, merged_rg_first) and author_last.casefold() == rg_tokens[-1].casefold() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = ['Dong', 'Ok', 'Kim']\n",
    "# b = \"Dong Ok Kim\"\n",
    "# c = \"Dong Wook Kim\"\n",
    "# compare_authors(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_it(driver, author_tokens, pairing_dict, author_id):\n",
    "    \"\"\"\n",
    "    Given the webdriver, parses its source for author URLs.\n",
    "    \n",
    "    Parameters:\n",
    "    driver == webdriver\n",
    "    author_tokens == list containing the searched author's name in tokens delimited by spaces\n",
    "    pairing_dict == Dictionary to store the URLs into (for the respective author indicated by author_id)\n",
    "    author_id == The searched author's id for usage in pairing_dict \n",
    "    \"\"\"    \n",
    "    \n",
    "    time.sleep(1) # Increased to 1 for stability\n",
    "      \n",
    "    if driver is None:\n",
    "        print(\"No soup for you: NO_PAPER\")\n",
    "        pairing_dict[author_id] = \"NO_PAPER\"\n",
    "        return -1\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    thing = soup.find_all('div', {\"class\": \"nova-v-person-list-item__title\"})\n",
    "    \n",
    "    print(\"Searching for: \" + str(author_tokens))\n",
    "    \n",
    "    match_count = 0\n",
    "    answer = \"\"\n",
    "    \n",
    "    for stuff in thing:     \n",
    "        author_name = stuff.find('a').string\n",
    "        author_url = stuff.find('a').get(\"href\")\n",
    "\n",
    "        if compare_authors(author_tokens, author_name):\n",
    "            print(str(author_url) + \" <---------------- \" + author_name)\n",
    "            answer = author_url\n",
    "            match_count += 1\n",
    "        else:\n",
    "            print(author_url + \" ~ \" + author_name)\n",
    "\n",
    "    if match_count == 0:\n",
    "        answer = \"NOT_FOUND2\"\n",
    "    elif match_count > 1:\n",
    "        answer = \"Duplicates\"\n",
    "        \n",
    "    print(\"Writing: \" + answer)\n",
    "    pairing_dict[author_id] = answer\n",
    "        \n",
    "    return match_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_file_descriptor = sys.stdout\n",
    "sys.stdout = open(\"./scraping_log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written from pairing_unknown_authors\n",
    "with open(\"./stored_authors/authors_and_papers.txt\", encoding=\"utf8\") as papers_file:\n",
    "    authors_and_papers = eval(papers_file.read())\n",
    "with open(\"./stored_authors/authors_ids.txt\", encoding=\"utf8\") as authors_file:\n",
    "    authors_ids = eval(authors_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authors_urls = {}\n",
    "\n",
    "# for author in authors_and_papers:\n",
    "#     authors_urls[author] = None\n",
    "\n",
    "with open(\"./stored_authors/author_url_pairings.txt\", encoding=\"utf8\") as author_pair_file:\n",
    "    authors_urls = eval(author_pair_file.read())\n",
    "\n",
    "# Any new entries not already saved in author_url_pairings.txt\n",
    "for author in authors_and_papers:\n",
    "    if author not in authors_urls:\n",
    "        authors_urls[author] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*****2019-07-15 09:59:38.427325*****\n",
      "\n",
      "Checking for linux64 geckodriver:v0.24.0 in cache\n",
      "Driver found in /home/local/NIST/jfl2/.wdm/geckodriver/v0.24.0/linux64/geckodriver\n",
      "TEST1\n",
      "TEST2\n",
      "TEST4\n",
      "\n",
      "---New search 2019-07-15 09:59:44.820607---\n",
      "DOI: 10.1016/j.memsci.2011.07.048\n",
      "Success: found at https://www.researchgate.net/publication/229313266_Characterization_and_gas_transport_properties_of_MOF-5_membranes\n",
      "No button\n",
      "DONE - 3.0114729404449463\n",
      "Searching for: ['Y.', 'S.', 'Lin']\n",
      "https://www.researchgate.net/profile/Zhenxia_Zhao ~ Zhenxia Zhao\n",
      "https://www.researchgate.net/scientific-contributions/82691542_Xiaoli_Ma ~ Xiaoli Ma\n",
      "https://www.researchgate.net/profile/Zhong_Li ~ Zhong Li\n",
      "https://www.researchgate.net/profile/Jerry_Lin8 <---------------- Jerry Y S Lin\n",
      "Writing: https://www.researchgate.net/profile/Jerry_Lin8\n",
      "0000-0001-5905-8336\n",
      "TEST4\n",
      "\n",
      "---New search 2019-07-15 10:00:04.518290---\n",
      "DOI: 10.1016/j.micromeso.2009.01.016\n",
      "Success: found at https://www.researchgate.net/publication/229331191_Atomically_detailed_models_of_gas_mixture_diffusion_through_CuBTC_membranes\n",
      "No button\n",
      "DONE - 3.0262796878814697\n",
      "Searching for: ['J.', 'Karl', 'Johnson']\n",
      "https://www.researchgate.net/profile/Seda_Keskin ~ Seda Keskin\n",
      "https://www.researchgate.net/scientific-contributions/30495748_Jinchen_Liu ~ Jinchen Liu\n",
      "https://www.researchgate.net/profile/Karl_Johnson13 ~ Karl Johnson\n",
      "https://www.researchgate.net/scientific-contributions/9750900_David_S_Sholl ~ David S. Sholl\n",
      "Writing: NOT_FOUND2\n",
      "0000-0002-3608-8003\n",
      "\n",
      "\n",
      "*****2019-07-15 10:00:22.842464*****\n",
      "\n",
      "Checking for linux64 geckodriver:v0.24.0 in cache\n",
      "Driver found in /home/local/NIST/jfl2/.wdm/geckodriver/v0.24.0/linux64/geckodriver\n"
     ]
    }
   ],
   "source": [
    "search_engine = \"https://www.google.com\"  # i.e. 0\n",
    "# search_engine = \"https://duckduckgo.com/\" # i.e. 1\n",
    "\n",
    "if search_engine == \"https://www.google.com\":\n",
    "    engine_number = 0\n",
    "elif search_engine == \"https://duckduckgo.com/\":\n",
    "    engine_number = 1\n",
    "    \n",
    "# replace_flag = \"NOT_FOUND\"\n",
    "replace_flag = None\n",
    "\n",
    "driver = obtain_driver(search_engine)\n",
    "print(\"TEST1\") #\n",
    "time.sleep(2)\n",
    "print(\"TEST2\") #\n",
    "\n",
    "for author in authors_urls:\n",
    "    if authors_urls[author] == replace_flag:\n",
    "        print(\"TEST4\") # \n",
    "        # Author has no associated paper\n",
    "        if len(authors_and_papers[author]) == 0:\n",
    "            print(\"\\n\\nAUTHOR HAS NO PAPER\")\n",
    "            authors_urls[author] = \"AUTHOR_HAS_NO_PAPER_IN_DATABASE\"\n",
    "            \n",
    "            driver.quit()\n",
    "            time.sleep(1)\n",
    "            driver = obtain_driver(search_engine)\n",
    "            time.sleep(2)\n",
    "            \n",
    "        else:\n",
    "            success = soup_it(show_authors(search_paper(authors_and_papers[author][0], driver, engine_number)), authors_ids[author][:3], authors_urls, author)\n",
    "            print(author)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if success == -1 or success == 0:\n",
    "                driver.quit()\n",
    "                time.sleep(1)\n",
    "                driver = obtain_driver(search_engine)\n",
    "                time.sleep(2)\n",
    "\n",
    "        time.sleep(random.randint(6, 12)) # Increased from 1 to 6-12 to respect crawling\n",
    "        \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./stored_authors/author_url_pairings.txt\", \"w\") as dup_file:\n",
    "    pprint(authors_urls, stream = dup_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = original_file_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997\n"
     ]
    }
   ],
   "source": [
    "print(len(authors_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*****2019-07-01 09:24:24.254224*****\n",
      "\n",
      "Checking for linux64 geckodriver:v0.24.0 in cache\n",
      "Driver found in /home/local/NIST/jfl2/.wdm/geckodriver/v0.24.0/linux64/geckodriver\n",
      "\n",
      "---New search 2019-07-01 09:24:31.453188---\n",
      "DOI: 10.1016/j.jssc.2012.08.046\n",
      "Success: found at https://www.researchgate.net/publication/256798038_Synthesis_of_MOF_having_hydroxyl_functional_side_groups_and_optimization_of_activation_process_for_the_maximization_of_its_BET_surface_area\n",
      "No button\n",
      "DONE - 2.0117602348327637\n",
      "Searching for: ['Dong', 'Ok', 'Kim']\n",
      "https://www.researchgate.net/scientific-contributions/2030683750_Jongsik_Kim ~ Jongsik Kim\n",
      "https://www.researchgate.net/profile/Dong_Kim30 <---------------- Dong Ok Kim\n",
      "https://www.researchgate.net/scientific-contributions/2153142096_Dong_Wook_Kim <---------------- Dong Wook Kim\n",
      "https://www.researchgate.net/scientific-contributions/2030803930_Kil_Sagong ~ Kil Sagong\n",
      "Writing: Duplicates\n"
     ]
    }
   ],
   "source": [
    "# # Testing Dong Kim (Wook vs Ok) duplicates\n",
    "\n",
    "# driver = obtain_driver(\"https://www.google.com\")\n",
    "# time.sleep(2)\n",
    "\n",
    "# author = \"dee98b81cb9252b78dc4cbe137a1e1983a19f047\"\n",
    "\n",
    "# soup_it(show_authors(search_paper(\"10.1016/j.jssc.2012.08.046\", driver, 0)), authors_ids[author], authors_urls, author)\n",
    "\n",
    "# driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
