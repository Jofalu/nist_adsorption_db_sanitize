{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_file_descriptor = sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = open(\"./scrape_log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------- STARTING ON \" + str(datetime.datetime.now()) + \" -------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a URL string, opens the URL in a headless Firefox instance and returns a reference to the webdriver object\n",
    "\n",
    "def obtain_driver(url):\n",
    "    buttons = []\n",
    "    driver_options = Options()\n",
    "    driver_options.headless = True\n",
    "    \n",
    "    print(\"***\" + str(datetime.datetime.now()) + \"***\")\n",
    "    driver = webdriver.Firefox(options=driver_options, executable_path=GeckoDriverManager().install())\n",
    "    # driver.install_addon(\"~/.mozilla/firefox/f4t6w0s5.default-release/extensions/uBlock0@raymondhill.net.xpi\", temporary=True)\n",
    "\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a DOI, uses obtain_driver to open up the search site and search the DOI to open up the paper in the page. Returns the webdriver associated with it.\n",
    "\n",
    "def search_paper(doi):\n",
    "    driver = obtain_driver(\"https://www.researchgate.net/search\")\n",
    "    \n",
    "    search_field = driver.find_elements_by_class_name(\"search-form__input\")[0]\n",
    "    search_field.send_keys(doi)\n",
    "    \n",
    "#     time.sleep(1)\n",
    "    \n",
    "    enter_field = driver.find_elements_by_class_name(\"search-form__left-button\")[0]\n",
    "    driver.execute_script(\"arguments[0].click()\", enter_field)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    if \"www.researchgate.net/search/\" in driver.current_url:\n",
    "        print(\"Paper not found (search)\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "    else: \n",
    "        return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the webdriver, shows more authors. Returns the modified webdriver.\n",
    "\n",
    "def show_authors(driver): \n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    if not driver:\n",
    "        print(\"Exiting: No driver\")\n",
    "        return 0\n",
    "    \n",
    "    first_start = time.time()\n",
    "    \n",
    "    buttons = driver.find_elements_by_class_name(\"show-more-authors\")\n",
    "    \n",
    "    print(\"show_more_authors button\")\n",
    "    print(buttons)\n",
    "    \n",
    "    # Need to address the condition of the button not existing (i.e. <= 4 authors for paper)\n",
    "    \n",
    "    if len(buttons) == 1:\n",
    "        time.sleep(3)\n",
    "        print(\"click\")\n",
    "        driver.execute_script(\"arguments[0].click();\", buttons[0])\n",
    "#         WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, \"show-less-authors\")))\n",
    "        time.sleep(3)    \n",
    "    \n",
    "    first_end = time.time()\n",
    "\n",
    "    print(\"DONE - \" + str(first_end-first_start))\n",
    "    \n",
    "    soup_it(driver)\n",
    "    \n",
    "    return 1\n",
    "    \n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the webdriver, parses its source for author URLs.\n",
    "\n",
    "def soup_it(driver):\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    thing = soup.find_all('div', {\"class\": \"nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp nova-v-person-list-item__title\"})\n",
    "    for stuff in thing:\n",
    "        print(stuff.find('a').get(\"href\"))\n",
    "    \n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on an example\n",
    "\n",
    "# source = search_paper(\"10.1021/acs.iecr.5b03509\")   # 6 authors\n",
    "source = search_paper(\"10.1007/s10450-014-9639-3\")  # 9 authors\n",
    "# source = search_paper(\"10.1002/adfm.200500561\")     # 4 authors\n",
    "# source = search_paper(\"10.1002/adfm.200500563\")     # doesn't exist\n",
    "\n",
    "# show_authors(source)\n",
    "\n",
    "# if exit_val != 0:\n",
    "searched = show_authors(source)\n",
    "# soup_it(searched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains author URLs from a basic request. Can't interact with the webpage to show more authors if there are > 4 authors.\n",
    "\n",
    "def obtain_author_researchgate(website_url):\n",
    "    request = urllib.request.Request(\n",
    "        url = website_url,\n",
    "        data = None,\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:67.0) Gecko/20100101 Firefox/67.0'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    testurl = urllib.request.urlopen(request)\n",
    "    soup = BeautifulSoup(testurl, 'html.parser')\n",
    "\n",
    "    thing = soup.find_all('div', {\"class\": \"nova-e-text nova-e-text--size-m nova-e-text--family-sans-serif nova-e-text--spacing-none nova-e-text--color-inherit nova-e-text--clamp nova-v-person-list-item__title\"})\n",
    "    for stuff in thing:\n",
    "        print(stuff.find('a').get(\"href\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
